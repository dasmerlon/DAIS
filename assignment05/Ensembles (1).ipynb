{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583d1489",
   "metadata": {},
   "source": [
    "# 1 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b86ad2",
   "metadata": {},
   "source": [
    "This task will be some hands-on programming of two different ensemble learning techniques, i.e. bagging and boosting. More specifically, we will use the [Bagging Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) as well as [AdaBoost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) provided by the scikit-learn library.\n",
    "\n",
    "As you are already familiar with loading datasets, splitting into training and test data, fitting a classifier etc., we will this time just give you hints about what steps to do. Also, we will give you some libraries to import, so that you have a good impression already about what you might need for this task. You can use additional imports of course, but please stick with the libraries we have used in this course so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad011a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Additional imports here, if needed\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df304a",
   "metadata": {},
   "source": [
    "## 1.1 Load and Split **Digits** Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcaef78",
   "metadata": {},
   "source": [
    "### Task 1.1.1 Load dataset \n",
    "\n",
    "Load the [Digits Toy Dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#digits-dataset) provided by Scikit-Learn and save it in the variable given below. Make sure that **all 10 digits** are included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdfe78d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0525e20",
   "metadata": {},
   "source": [
    "### Task 1.1.2 Save feature and target data\n",
    "\n",
    "Save the feature data from the dataset (i.e. the vectors representing the digits) and the respective target labels in two different variables *X* and *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "075926da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797\n",
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "X = digits[\"data\"]\n",
    "y = digits[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1322257b",
   "metadata": {},
   "source": [
    "### Task 1.1.3 Split train and test data\n",
    "\n",
    "Split all data into train and test set, denoting your training set *(X_train, y_train)* and your test set *(X_test, y_test)*. We want to have 70% of the samples for training and 30% of the samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7969cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  1257 1257 [array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  3.,  0.,  0.],\n",
      "       [ 0.,  0.,  5., ..., 11.,  3.,  0.],\n",
      "       [ 0.,  0.,  3., ...,  0.,  0.,  0.]]), array([0, 1, 2, ..., 4, 3, 1])]\n",
      "TEST:  540 540 [array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  5., ...,  3.,  0.,  0.],\n",
      "       [ 0.,  7., 16., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
      "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
      "       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), array([4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4,\n",
      "       8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n",
      "       4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5,\n",
      "       0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2,\n",
      "       6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0,\n",
      "       1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0,\n",
      "       5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8,\n",
      "       4, 9, 0, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "       8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9,\n",
      "       8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7,\n",
      "       3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3,\n",
      "       2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9,\n",
      "       6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8,\n",
      "       9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1,\n",
      "       2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7,\n",
      "       7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6,\n",
      "       6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3,\n",
      "       1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4,\n",
      "       7, 2, 2, 5, 7, 9, 5, 4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7,\n",
      "       8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
      "       0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2,\n",
      "       7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9,\n",
      "       5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6,\n",
      "       8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5,\n",
      "       7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8])]\n"
     ]
    }
   ],
   "source": [
    "def split(data, target, percentage):\n",
    "    train = [data[:int(len(X) * percentage)], target[:int(len(X) * percentage)]]\n",
    "    test = [data[int(len(X) * percentage):], target[int(len(X) * percentage):]]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train_set, test_set = split(X, y, 0.7)\n",
    "print(\"TRAIN: \", len(train_set[0]), len(train_set[1]), train_set)\n",
    "print(\"TEST: \", len(test_set[0]), len(test_set[1]), test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3a37e",
   "metadata": {},
   "source": [
    "## Task 1.2 Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a684cb0",
   "metadata": {},
   "source": [
    "### Task 1.2.1 Train Classifier\n",
    "\n",
    "Create a **Bagging Classifier** object and train it on the given training data you got from task 1.1.3. You can play with different sets of parameters for the classifier (such as number of estimators etc.), but please keep the original base estimator (i.e. decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26b33171",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BaggingClassifier()\n",
    "\n",
    "bc_model = bc.fit(train_set[0], train_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6aed0d",
   "metadata": {},
   "source": [
    "### Task 1.2.2 Evaluate Classifier\n",
    "\n",
    "Now it's time to see how the classifier performs. Make the prediction of target labels *y_pred* based on the test samples and print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "907a3964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier on Digits Dataset: 87.7778\n"
     ]
    }
   ],
   "source": [
    "y_pred = bc_model.predict(test_set[0])\n",
    "\n",
    "bc_accuracy = bc_model.score(test_set[0], test_set[1])\n",
    "\n",
    "print(\"Accuracy of Bagging Classifier on Digits Dataset: %.4f\" % (100 * bc_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2cb57",
   "metadata": {},
   "source": [
    "Great, you're done! Now it's time to do the same few steps using the AdaBoost classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3ad56",
   "metadata": {},
   "source": [
    "## Task 1.3 AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890d1ce",
   "metadata": {},
   "source": [
    "### Task 1.3.1 Train Classifier\n",
    "\n",
    "Create an **AdaBoost Classifier** object and train it on the given training data you got from task 1.1.3. You can play with different sets of parameters for the classifier (such as number of estimators, learning rate etc.), but please keep the original base estimator (i.e. decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07eab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = # your code here (AdaBoost classifier object)\n",
    "\n",
    "abc_model = # your code here (trained classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50442418",
   "metadata": {},
   "source": [
    "### Task 1.3.2 Evaluate Classifier\n",
    "\n",
    "Once again it's time to see how the classifier performs. Make the prediction of target labels *y_pred* based on the test samples and print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f853f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = # your code here\n",
    "\n",
    "abc_accuracy = # your code here\n",
    "\n",
    "print(\"Accuracy of AdaBoost Classifier on Digits Dataset: %.4f\" % abc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da78ccc",
   "metadata": {},
   "source": [
    "_________________________\n",
    "\n",
    "You're done! Are the accuracy results as you had expected? If not, feel free to adjust some parameters to maximize your outcome. Either way, you might have just written your very first ensemble classifiers in python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
